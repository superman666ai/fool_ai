二．特征工程

特征工程的目的是把原始的数据转化为我们的模型可以使用的数据，其主要包括三个子问题，特征构造、特征提取和特征选择。特征构造一般是在原有特征的基础上做一些“组合”操作，例如对原有特征进行四则运算，从而得到新的特征。特征提取是指使用映射或变换的方法将维数较高的原始特征转换为维数较低的新的特征。特征选择即从原始的特征中挑选出一些最具有代表性，使得模型效果最好的特征。其中特征提取和特征选择最常使用。

1. 特征提取

特征提取又叫作“降维”，目前对于线性特征的提取，常用方法有主成分分析（PrincipleComponent Analysis，PCA）、线性判别分析（LinearDiscriminant Analysis，LDA）以及独立成分分析（Independent Component Analysis，ICA）。

①   主成分分析（Principle Component Analysis，PCA）[1]

主成分分析（PCA）是一种经典的无监督降维方法，它的主要思想是在降维的过程中实现“减少噪声”和“去冗余”从而达到降维的目的。具体来说，“减少噪声”是指在将维数较高的原始特征转换为维数较低的新特征的过程中保留下维度间相关性尽可能小的特征维度，这一操作实际上是通过借助协方差矩阵的原理所实现的；“去冗余”是指把“减少噪声”操作之后保留下来的维度再进一步筛选，去掉含有较小“特征值”的维度，使得留下来的特征维度含有的“特征值”尽可能的大，特征值越大方差就会越大，进而所包含的信息量就会越大。（注：关于为什么通过借助协方差原理可以实现将维数较高的原始特征转换为理想的维数较低的新特征，也就是特征提取，不在本书的讨论范围内）。




主成分分析（PCA）法一大特点就是它是完全无参数限制的，也就是说PCA的结果只与数据有关，而用户是无法进行干预的。这是它的优点，同时也是缺点。针对这一特点，PCA核方法kernel-PCA后来被提出，使得用户可以根据先验知识预先对数据进行非线性转换，它也是当下比较流行的方法之一。




②   线性判别分析（Linear Discriminant Analysis，LDA）[2]

线性判别分析（LDA）是一种经典的有监督降维算法，它的主要思想是借助协方差矩阵、广义瑞利熵等原理实现数据类别间距离的最大化和类别内距离的最小化。了解了其主要思想，那么线性判别分析（LDA）又是怎样实现的呢？这里以二分类LDA为例，二维特征通过一系列矩阵运算实现从二维平面到一条直线的投影，期间同时通过借助协方差矩阵、广义瑞利熵等实现类间数据的最大化与类内数据的最小化。从二分类推广到多分类是在二分类的基础上增加了“全局散度矩阵”来实现最终目标优化函数的设定，从而实现类间距离的最大化和类内距离的最小化。显然，由于它是针对各个类别做的降维，所以数据经过线性判别分析（LDA）降维后，最多只能降到原来的类别数减一的维度。




正因为如上特性，线性判别分析（LDA）除了可以实现降维外还可以实现分类[3]。另外，对比前文讲的主成分分析（PCA）可以看出，LDA在降维过程中着重考虑分类性能，而PCA着重考虑特征维度之间的差异性与方差的大小即信息量的大小。




③   独立成分分析（Independent Component Analysis，ICA）[4]

独立成分分析（ICA）的主要思想是在降维的过程中保留相互独立的特征维度。这比PCA更进一步，在保证特征维度之间不相关的同时保证相互独立。不相关只是保证了没有线性关系，而并不能保证是独立的。




正因为其是以保证特征维度之间的相互独立性为目标，独立成分分析（ICA）往往会比PCA有更好的降维效果。独立成分分析（ICA）目前已经广泛的应用到数据挖掘、图像处理等多个领域。




2. 特征选择

不同的特征对模型的影响程度不同，我们要选择出重要的一些特征，移除与问题相关性不是很大的特征，这个过程就叫做特征选择。特征选择的最终目的是通过减少冗余特征以达到减少过拟合、提高模型准确度和在一定程度上减少训练时间的效果。对比前文介绍的特征提取，特征选择是对原始特征取特征子集的一个操作，而特征提取则是对原始特征进行映射或者变换操作以得到低维的新特征。



特征的选择在特征工程中十分重要，往往可以很大程度上决定最后模型训练结果的好坏。常用的特征选择方法有：过滤式（filter）、包裹式（wrapper）以及嵌入式（embedding）。

① 过滤式

过滤式特征选择一般是通过统计度量的方法评估每个特征和结果的相关性，来对特征进行筛选，留下相关性较强的特征。其核心思想是：先对数据集进行特征选择，然后再进行模型的训练。即过滤式特征选择是独立于特定的学习算法的。也正因如此，过滤式特征选择拥有较高的通用性，可适用于大规模数据集。同样地，正是由于其独立于特定的学习算法，也造成了其在后面的模型表现即分类准确率方面可能会表现欠佳。常用的过滤式特征选择方法有Pearson相关系数法、方差选择法、假设检验、互信息法等。这些方法通常是单变量的。

② 包裹式

包裹式特征选择通常是把最终机器学习模型的表现作为特征选择的重要依据，一步步筛选特征。这种一步步筛选特征的过程可以看作是目标特征组合的搜索过程，这种搜索过程可以是最佳优先搜索、随机爬山算法等。目前比较常用的一种包裹式特征选择法为递归特征消除法，其原理是使用一个基模型（如：随机森林、逻辑回归等）进行多轮训练，每轮训练结束后，消除若干权值系数较低的特征，再基于新的特征集进行新的一轮训练。




正是由于包裹式特征选择是根据最终的模型表现来选择特征的，故通常其要比前文提到的过滤式特征选择有着更好的模型表现。同样的，由于训练过程时间久，系统的开销也更大，一般来说不太适用于大规模数据集。

③  嵌入式

嵌入式特征选择同样是根据机器学习的算法、模型来分析特征的重要性，从而选择比较重要的N个特征。与包裹式特征选择法最大的不同是，嵌入式方法是将特征选择过程与模型的训练过程结合为一体，这样就可以快速地找到最佳的特征集合，更加高效、快捷。简而言之，嵌入式特征选择是将全部的数据一起输入模型中进行训练和评测，而包裹式特征选择一般是一步步筛选特征，一步步减少特征进而得到所需要的特征维度。常用的嵌入式特征选择方法有基于正则化项的特征选择法（如：Lasso）和基于树模型的特征选择法（如：GBDT）。
